{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv(\"/Users/shubhamgandhi/Desktop/aml/Churn-Prediction/preprocessed_with_smote.csv\")\n",
    "data_cleaned = data.drop([], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_cleaned.drop(\"Exited\", axis=1)\n",
    "y = data_cleaned[\"Exited\"]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize base models\n",
    "random_forest = RandomForest( n_estimators=100)\n",
    "gboost = GradientBoosting( n_estimators=100)\n",
    "\n",
    "# Prepare stacking features\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "stack_train_list = []  # List to collect stacking training data\n",
    "stack_test = pd.DataFrame()\n",
    "stack_test_preds = []\n",
    "\n",
    "# Train base models using k-fold cross-validation for stacking\n",
    "for train_idx, valid_idx in skf.split(X_train, y_train):\n",
    "    X_fold_train, X_fold_valid = X_train[train_idx], X_train[valid_idx]\n",
    "    y_fold_train, y_fold_valid = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "    \n",
    "    # Train Random Forest and Gradient Boosting\n",
    "    random_forest.fit(X_fold_train, y_fold_train)\n",
    "    gboost.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    # Generate predictions for stacking\n",
    "    stack_train_list.append(pd.DataFrame({\n",
    "        \"rf_pred\": random_forest.predict_proba(X_fold_valid)[:, 1],\n",
    "        \"gboost_pred\": gboost.predict_proba(X_fold_valid)[:, 1],\n",
    "        \"target\": y_fold_valid.values\n",
    "    }))\n",
    "    \n",
    "    # Collect test predictions for averaging\n",
    "    stack_test_preds.append({\n",
    "        \"rf_pred\": random_forest.predict_proba(X_test)[:, 1],\n",
    "        \"gboost_pred\": gboost.predict_proba(X_test)[:, 1],\n",
    "    })\n",
    "\n",
    "# Concatenate all stacking data into a single DataFrame\n",
    "stack_train = pd.concat(stack_train_list, ignore_index=True)\n",
    "\n",
    "# Average test predictions from each fold\n",
    "stack_test[\"rf_pred\"] = sum([pred[\"rf_pred\"] for pred in stack_test_preds]) / len(stack_test_preds)\n",
    "stack_test[\"gboost_pred\"] = sum([pred[\"gboost_pred\"] for pred in stack_test_preds]) / len(stack_test_preds)\n",
    "\n",
    "# Separate training data for the meta-model\n",
    "X_meta = stack_train[[\"rf_pred\", \"gboost_pred\"]]\n",
    "y_meta = stack_train[\"target\"]\n",
    "\n",
    "# Train Logistic Regression as the meta-model\n",
    "logistic_regression = LogisticRegression(random_state=42)\n",
    "logistic_regression.fit(X_meta, y_meta)\n",
    "\n",
    "# Make predictions on the meta test set\n",
    "meta_test_preds = logistic_regression.predict_proba(stack_test)[:, 1]\n",
    "meta_test_labels = (meta_test_preds > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the performance of the stacked model\n",
    "stacked_accuracy = accuracy_score(y_test, meta_test_labels)\n",
    "print(f\"Stacked Model Accuracy: {stacked_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, meta_test_labels))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(y_test, meta_test_labels)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.matshow(conf_matrix, cmap='Blues', fignum=1)\n",
    "plt.title(\"Confusion Matrix\", pad=20)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "for (i, j), value in np.ndenumerate(conf_matrix):\n",
    "    plt.text(j, i, f\"{value}\", ha=\"center\", va=\"center\")\n",
    "plt.show()\n",
    "\n",
    "# ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, meta_test_preds)\n",
    "print(f\"ROC AUC Score: {roc_auc:.2f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, meta_test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"Stacked Model (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Guess\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.special import expit  # for sigmoid if needed\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Decision Tree and PreSortedDecisionTree Classes\n",
    "# -----------------------------------------------\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return np.round(np.mean(y))  # Majority class as leaf\n",
    "\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return y.iloc[0]\n",
    "\n",
    "        if X.shape[1] == 0:\n",
    "            return np.round(np.mean(y))\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return np.round(np.mean(y))\n",
    "\n",
    "        left_idx = X[best_feature] <= best_threshold\n",
    "        right_idx = ~left_idx\n",
    "\n",
    "        left_tree = self.fit(X[left_idx], y[left_idx], depth + 1)\n",
    "        right_tree = self.fit(X[right_idx], y[right_idx], depth + 1)\n",
    "\n",
    "        return {\"feature\": best_feature, \"threshold\": best_threshold, \"left\": left_tree, \"right\": right_tree}\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        raise NotImplementedError(\"This method should be implemented in the subclass.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.apply(self._predict_row, axis=1, tree=self.tree)\n",
    "\n",
    "    def _predict_row(self, row, tree):\n",
    "        if isinstance(tree, dict):\n",
    "            if row[tree[\"feature\"]] <= tree[\"threshold\"]:\n",
    "                return self._predict_row(row, tree[\"left\"])\n",
    "            else:\n",
    "                return self._predict_row(row, tree[\"right\"])\n",
    "        else:\n",
    "            return tree\n",
    "\n",
    "\n",
    "class PreSortedDecisionTree(DecisionTree):\n",
    "    def _best_split(self, X, y):\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in X.columns:\n",
    "            sorted_indices = np.argsort(X[feature])\n",
    "            X_sorted, y_sorted = X.iloc[sorted_indices], y.iloc[sorted_indices]\n",
    "\n",
    "            for i in range(1, len(y_sorted)):\n",
    "                if X_sorted[feature].iloc[i] == X_sorted[feature].iloc[i - 1]:\n",
    "                    continue\n",
    "\n",
    "                threshold = (X_sorted[feature].iloc[i] + X_sorted[feature].iloc[i - 1]) / 2\n",
    "                left_idx = X_sorted[feature] <= threshold\n",
    "                right_idx = ~left_idx\n",
    "\n",
    "                if len(y_sorted[left_idx]) == 0 or len(y_sorted[right_idx]) == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = self._information_gain(y_sorted, y_sorted[left_idx], y_sorted[right_idx])\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    @staticmethod\n",
    "    def _information_gain(parent, left, right):\n",
    "        def entropy(y):\n",
    "            probabilities = y.value_counts(normalize=True)\n",
    "            return -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "\n",
    "        parent_entropy = entropy(parent)\n",
    "        left_entropy = entropy(left)\n",
    "        right_entropy = entropy(right)\n",
    "\n",
    "        n = len(parent)\n",
    "        n_left = len(left)\n",
    "        n_right = len(right)\n",
    "\n",
    "        weighted_avg_entropy = (n_left / n) * left_entropy + (n_right / n) * right_entropy\n",
    "        return parent_entropy - weighted_avg_entropy\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Custom Random Forest Implementation with predict_proba\n",
    "# ---------------------------------------------------\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=None, max_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_features = X.shape[1]\n",
    "        max_features = self.max_features or int(np.sqrt(n_features))\n",
    "\n",
    "        for _ in range(self.n_trees):\n",
    "            indices = np.random.choice(len(X), len(X), replace=True)\n",
    "            X_sample, y_sample = X.iloc[indices], y.iloc[indices]\n",
    "\n",
    "            features = random.sample(list(X.columns), max_features)\n",
    "            X_sample = X_sample[features]\n",
    "\n",
    "            tree = PreSortedDecisionTree(max_depth=self.max_depth)\n",
    "            tree.tree = tree.fit(X_sample, y_sample)\n",
    "\n",
    "            self.trees.append((tree, features))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for tree, features in self.trees:\n",
    "            X_subset = X[features]\n",
    "            predictions.append(tree.predict(X_subset))\n",
    "\n",
    "        predictions = np.array(predictions)  # shape (n_trees, n_samples)\n",
    "        # Majority vote\n",
    "        majority_vote = np.round(predictions.mean(axis=0))\n",
    "        return majority_vote\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Probability as mean prediction across trees\n",
    "        # Each tree returns 0 or 1 predictions\n",
    "        all_preds = []\n",
    "        for tree, features in self.trees:\n",
    "            X_subset = X[features]\n",
    "            preds = tree.predict(X_subset).values.astype(float)\n",
    "            all_preds.append(preds)\n",
    "        all_preds = np.array(all_preds)\n",
    "        prob_class_1 = all_preds.mean(axis=0)\n",
    "        prob_class_0 = 1 - prob_class_1\n",
    "        return np.vstack((prob_class_0, prob_class_1)).T\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# OptimizedDecisionTree for Gradient Boosting\n",
    "# -------------------------------------------------\n",
    "class OptimizedDecisionTree:\n",
    "    \"\"\"\n",
    "    A more efficient implementation of a regression tree for Gradient Boosting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=3, min_samples_split=2, num_thresholds=10):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.num_thresholds = num_thresholds\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        if depth == self.max_depth or n_samples < self.min_samples_split:\n",
    "            return np.mean(y)  # Return leaf value\n",
    "        \n",
    "        best_split = self.find_best_split(X, y, n_features)\n",
    "        if not best_split:\n",
    "            return np.mean(y)\n",
    "        \n",
    "        feature, threshold, left_idx, right_idx = best_split\n",
    "        left_tree = self.fit(X[left_idx], y[left_idx], depth + 1)\n",
    "        right_tree = self.fit(X[right_idx], y[right_idx], depth + 1)\n",
    "        \n",
    "        return {\"feature\": feature, \"threshold\": threshold, \"left\": left_tree, \"right\": right_tree}\n",
    "\n",
    "    def find_best_split(self, X, y, n_features):\n",
    "        best_feature, best_threshold = None, None\n",
    "        best_variance = float(\"inf\")\n",
    "        best_left_idx, best_right_idx = None, None\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            feature_values = X[:, feature]\n",
    "            thresholds = np.percentile(feature_values, np.linspace(0, 100, self.num_thresholds))\n",
    "            for threshold in thresholds:\n",
    "                left_idx = feature_values <= threshold\n",
    "                right_idx = feature_values > threshold\n",
    "                if np.sum(left_idx) == 0 or np.sum(right_idx) == 0:\n",
    "                    continue\n",
    "                \n",
    "                left_variance = np.var(y[left_idx]) * np.sum(left_idx)\n",
    "                right_variance = np.var(y[right_idx]) * np.sum(right_idx)\n",
    "                total_variance = left_variance + right_variance\n",
    "\n",
    "                if total_variance < best_variance:\n",
    "                    best_variance = total_variance\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    best_left_idx = left_idx\n",
    "                    best_right_idx = right_idx\n",
    "\n",
    "        if best_feature is None:\n",
    "            return None\n",
    "        return best_feature, best_threshold, best_left_idx, best_right_idx\n",
    "\n",
    "    def predict_single(self, x, tree):\n",
    "        if not isinstance(tree, dict):  # Leaf node\n",
    "            return tree\n",
    "        feature = tree[\"feature\"]\n",
    "        threshold = tree[\"threshold\"]\n",
    "        if x[feature] <= threshold:\n",
    "            return self.predict_single(x, tree[\"left\"])\n",
    "        else:\n",
    "            return self.predict_single(x, tree[\"right\"])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_single(x, self.tree) for x in X])\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Gradient Boosting Custom Class\n",
    "# ---------------------------------------------\n",
    "class GradientBoostingCustom:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.01, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.initial_prediction = None\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        predictions = np.full(len(y), self.initial_prediction)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - predictions\n",
    "            tree = OptimizedDecisionTree(max_depth=self.max_depth, num_thresholds=10)\n",
    "            tree.tree = tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        preds = np.full(X.shape[0], self.initial_prediction)\n",
    "        for tree in self.trees:\n",
    "            preds += self.learning_rate * tree.predict(X)\n",
    "        # Convert continuous predictions to binary (threshold at 0.5)\n",
    "        return (preds >= 0.5).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # We'll apply a sigmoid to continuous predictions to convert them to probabilities\n",
    "        X = np.array(X)\n",
    "        preds = np.full(X.shape[0], self.initial_prediction)\n",
    "        for tree in self.trees:\n",
    "            preds += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        # Use sigmoid to ensure results are between 0 and 1\n",
    "        prob_class_1 = expit(preds)\n",
    "        prob_class_0 = 1 - prob_class_1\n",
    "        return np.vstack((prob_class_0, prob_class_1)).T\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main code: Data loading, stacking, evaluation\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    data = pd.read_csv(\"/Users/shubhamgandhi/Desktop/aml/Churn-Prediction/preprocessed_with_smote.csv\")\n",
    "\n",
    "    # Separate features and target\n",
    "    X = data.drop(\"Exited\", axis=1)\n",
    "    y = data[\"Exited\"]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.30, random_state=42, stratify=y)\n",
    "\n",
    "    # Initialize base models\n",
    "    random_forest = RandomForest(n_trees=100)\n",
    "    gboost = GradientBoostingCustom(n_estimators=100, learning_rate=0.01, max_depth=3)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    stack_train_list = []\n",
    "    stack_test_preds = []\n",
    "\n",
    "    # Stacking\n",
    "    for train_idx, valid_idx in skf.split(X_train, y_train):\n",
    "        X_fold_train, X_fold_valid = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "        y_fold_train, y_fold_valid = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "\n",
    "        # Fit base models\n",
    "        random_forest.fit(X_fold_train, y_fold_train)\n",
    "        gboost.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "        # Out-of-fold predictions\n",
    "        rf_val_preds = random_forest.predict_proba(X_fold_valid)[:, 1]\n",
    "        gb_val_preds = gboost.predict_proba(X_fold_valid)[:, 1]\n",
    "\n",
    "        fold_df = pd.DataFrame({\n",
    "            \"rf_pred\": rf_val_preds,\n",
    "            \"gboost_pred\": gb_val_preds,\n",
    "            \"target\": y_fold_valid.values\n",
    "        })\n",
    "        stack_train_list.append(fold_df)\n",
    "\n",
    "        # Predictions on the test set\n",
    "        rf_test_preds = random_forest.predict_proba(X_test)[:, 1]\n",
    "        gb_test_preds = gboost.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        stack_test_preds.append({\n",
    "            \"rf_pred\": rf_test_preds,\n",
    "            \"gboost_pred\": gb_test_preds,\n",
    "        })\n",
    "\n",
    "    stack_train = pd.concat(stack_train_list, ignore_index=True)\n",
    "\n",
    "    # Average test predictions from each fold\n",
    "    stack_test = pd.DataFrame()\n",
    "    stack_test[\"rf_pred\"] = sum([pred[\"rf_pred\"] for pred in stack_test_preds]) / len(stack_test_preds)\n",
    "    stack_test[\"gboost_pred\"] = sum([pred[\"gboost_pred\"] for pred in stack_test_preds]) / len(stack_test_preds)\n",
    "\n",
    "    # Train meta-model (Logistic Regression)\n",
    "    X_meta = stack_train[[\"rf_pred\", \"gboost_pred\"]]\n",
    "    y_meta = stack_train[\"target\"]\n",
    "\n",
    "    logistic_regression = LogisticRegression(random_state=42)\n",
    "    logistic_regression.fit(X_meta, y_meta)\n",
    "\n",
    "    # Meta predictions\n",
    "    meta_test_preds = logistic_regression.predict_proba(stack_test)[:, 1]\n",
    "    meta_test_labels = (meta_test_preds > 0.5).astype(int)\n",
    "\n",
    "    # Evaluate\n",
    "    stacked_accuracy = accuracy_score(y_test, meta_test_labels)\n",
    "    print(f\"Stacked Model Accuracy: {stacked_accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
